import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import pandas as pd

# é€™å€‹æ¨¡å‹æ˜¯ OpenAI é‡‹å‡ºçš„ GPT-2 åµæ¸¬å™¨
# id2label: 0 -> "Fake" (æ¯”è¼ƒåƒ GPT-2 ç”¢ç”Ÿ), 1 -> "Real" (æ¯”è¼ƒåƒäººé¡æ’°å¯«)
MODEL_NAME = "openai-community/roberta-base-openai-detector"


@st.cache_resource
def load_model():
    """åªåœ¨ç¬¬ä¸€æ¬¡å‘¼å«æ™‚ä¸‹è¼‰ / è¼‰å…¥æ¨¡å‹ï¼Œä¹‹å¾Œé‡ç”¨åŒä¸€ä»½æ¬Šé‡ã€‚"""
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)
    model.eval()
    return tokenizer, model


def predict_proba(text: str, tokenizer, model):
    """å°ä¸€æ®µæ–‡å­—åšæ¨è«–ï¼Œå›å‚³ (ai_prob, human_prob)ã€‚"""
    # å°‡æ–‡å­—ç·¨ç¢¼æˆå¼µé‡ï¼ˆæœ€å¤š 512 token å°±å¥½ï¼‰
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=512,
    )

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits[0]  # shape: [2]

    probs = torch.softmax(logits, dim=-1).tolist()
    # æ ¹æ“š config.json: 0 = "Fake" (AI-like), 1 = "Real" (Human-like)
    ai_prob = probs[0]
    human_prob = probs[1]
    return ai_prob, human_prob


def main():
    st.set_page_config(
        page_title="AI / Human æ–‡ç« åµæ¸¬å™¨",
        page_icon="ğŸ¤–",
        layout="centered",
    )

    st.title("ğŸ¤– AI / Human æ–‡ç« åµæ¸¬å™¨ (Demo)")
    st.write(
        """
        è¼¸å…¥ä¸€æ®µæ–‡å­—ï¼Œæ¨¡å‹æœƒä¼°è¨ˆé€™æ®µæ–‡å­—æ¯”è¼ƒåƒæ˜¯ **AI ç”Ÿæˆ** é‚„æ˜¯ **äººé¡æ’°å¯«**ï¼Œ
        ä¸¦çµ¦å‡ºå°æ‡‰çš„æ©Ÿç‡ç™¾åˆ†æ¯”ï¼ˆAI% / Human%ï¼‰ã€‚
        """
    )
    st.caption(
        "âš ï¸ æœ¬å·¥å…·åƒ…ä¾›èª²ç¨‹ / ç ”ç©¶ **æ•™å­¸ç¤ºç¯„**ï¼Œ"
        "æº–ç¢ºç‡æœ‰é™ï¼Œè«‹å‹¿ä½œç‚ºå­¸è¡“é•è¦æˆ–æŠ„è¥²åˆ¤å®šçš„å”¯ä¸€ä¾æ“šã€‚"
    )

    with st.sidebar:
        st.header("è¨­å®š")
        st.markdown(
            f"**åµæ¸¬æ¨¡å‹ï¼š** `{MODEL_NAME}`  \n"
            "é€™æ˜¯ RoBERTa base æ¨¡å‹å¾®èª¿è€Œæˆçš„ GPT-2 æ–‡å­—åµæ¸¬å™¨ã€‚"
        )
        threshold = st.slider(
            "åˆ¤å®šç‚ºã€ŒAI ç”Ÿæˆã€çš„é–€æª» (AI%)",
            min_value=0.0,
            max_value=1.0,
            value=0.5,
            step=0.01,
        )
        st.caption(
            "ä¾‹å¦‚é–€æª» = 0.7ï¼Œä»£è¡¨ AI æ©Ÿç‡ â‰¥ 70% æ™‚ï¼Œå°±é¡¯ç¤ºç‚ºã€Œè¼ƒåƒ AIã€ã€‚"
        )

    st.subheader("è¼¸å…¥å¾…æª¢æ¸¬æ–‡å­—")
    text = st.text_area(
        label="è«‹åœ¨é€™è£¡è²¼ä¸Šè¦åµæ¸¬çš„æ–‡ç« ï¼ˆä¸­è‹±æ–‡çš†å¯ï¼‰ï¼š",
        height=200,
        placeholder="ä¾‹å¦‚ï¼šæœ¬ç ”ç©¶æ—¨åœ¨æ¢è¨......",
    )

    # è¼‰å…¥æ¨¡å‹ï¼ˆç¬¬ä¸€æ¬¡æœƒä¸‹è¼‰ & loadï¼Œéœ€è¦ä¸€é»æ™‚é–“ï¼‰
    tokenizer, model = load_model()

    if text.strip():
        # åšæ¨è«–
        ai_prob, human_prob = predict_proba(text, tokenizer, model)
        ai_pct = ai_prob * 100
        human_pct = human_prob * 100

        # æ–‡å­—åˆ¤æ–·çµæœ
        st.subheader("åˆ¤æ–·çµæœ")

        if ai_prob >= threshold:
            label_text = "çœ‹èµ·ä¾† **è¼ƒåƒ AI ç”¢ç”Ÿçš„å…§å®¹** ğŸ¤–"
        else:
            label_text = "çœ‹èµ·ä¾† **è¼ƒåƒäººé¡æ’°å¯«çš„å…§å®¹** ğŸ§‘"

        st.markdown(
            f"""
            ### {label_text}

            - **AI æ©Ÿç‡ (Fake / Model-Generated)**ï¼š`{ai_pct:.2f}%`  
            - **Human æ©Ÿç‡ (Real / Human-Written)**ï¼š`{human_pct:.2f}%`
            """
        )

        # ç°¡å–®è¦–è¦ºåŒ–ï¼šé•·æ¢åœ–é¡¯ç¤º AI% / Human%
        st.subheader("æ©Ÿç‡åˆ†ä½ˆ (AI% vs Human%)")
        df = pd.DataFrame(
            {
                "é¡åˆ¥": ["AI ç”Ÿæˆ", "Human"],
                "æ©Ÿç‡ç™¾åˆ†æ¯”": [ai_pct, human_pct],
            }
        )
        st.bar_chart(df.set_index("é¡åˆ¥"))

        # é¡å¤–è³‡è¨Šï¼šä¸€äº›ç°¡å–®çµ±è¨ˆé‡ï¼ˆå¯é¸ï¼‰
        st.subheader("æ–‡å­—çµ±è¨ˆ (é¸ç”¨)")
        num_chars = len(text)
        num_words = len(text.split())
        num_lines = len(text.splitlines())

        col1, col2, col3 = st.columns(3)
        col1.metric("å­—å…ƒæ•¸ (characters)", num_chars)
        col2.metric("è©æ•¸ (words, ä»¥ç©ºç™½åˆ‡)", num_words)
        col3.metric("è¡Œæ•¸ (lines)", num_lines)

        with st.expander("æ¨¡å‹æŠ€è¡“èªªæ˜ / é™åˆ¶ï¼ˆå¯ä»¥å¯«åœ¨å ±å‘Šèªªæ˜ï¼‰"):
            st.markdown(
                """
                - ä½¿ç”¨çš„æ¨¡å‹ï¼š`openai-community/roberta-base-openai-detector`  
                  - é€™å€‹æ¨¡å‹æ˜¯ RoBERTa base å¾®èª¿è€Œæˆï¼Œç”¨ä¾†åˆ†è¾¨æ–‡å­—æ˜¯å¦ç”± GPT-2 ç”¢ç”Ÿã€‚:contentReference[oaicite:1]{index=1}  
                - æ¨¡å‹è¼¸å‡ºå…©å€‹é¡åˆ¥ï¼š  
                  - `Fake`ï¼šè¼ƒå¯èƒ½æ˜¯ **æ¨¡å‹ç”¢ç”Ÿ (AI-like)**  
                  - `Real`ï¼šè¼ƒå¯èƒ½æ˜¯ **äººé¡æ’°å¯« (Human-like)**  
                - æˆ‘å€‘å°‡ `Fake` è¦–ç‚º AI ç”Ÿæˆæ©Ÿç‡ (AI%)ï¼Œ`Real` è¦–ç‚º Human æ©Ÿç‡ (Human%)ã€‚  
                - ç”±æ–¼è¨“ç·´è³‡æ–™ä¸»è¦ä¾†è‡ª GPT-2 çš„è¼¸å‡ºï¼Œå° ChatGPT / GPT-4 ç­‰è¼ƒæ–°æ¨¡å‹ï¼Œ
                  åµæ¸¬æ•ˆæœæœ‰é™ï¼Œåªèƒ½åš**å‚¾å‘æ€§åˆ¤æ–·**ï¼Œä¸æ˜¯çµ•å°äº‹å¯¦ã€‚:contentReference[oaicite:2]{index=2}
                """
            )
    else:
        st.info("ğŸ‘† è«‹å…ˆåœ¨ä¸Šæ–¹è¼¸å…¥ä¸€æ®µæ–‡å­—ï¼Œç³»çµ±æ‰æœƒé€²è¡Œ AI / Human åˆ¤æ–·ã€‚")


if __name__ == "__main__":
    main()
